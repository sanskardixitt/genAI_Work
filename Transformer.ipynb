{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d22asSHa60mO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
        "import numpy as np\n",
        "\n",
        "#load the harry potter book as the dataset ->  url - https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books\n",
        "def load_data(file_path):\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "  return text\n",
        "\n",
        "file_path = \"hp_1.txt\"\n",
        "text = load_data(file_path).lower()\n",
        "\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# Convert text to sequences\n",
        "input_sequences = []\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "seq_length = 50\n",
        "\n",
        "# First seq_length tokens (input): Used for training the model.\n",
        "# Last token (target): Used as the label the model tries to predict.\n",
        "# so total of (50 + 1) in one input_sequence index\n",
        "\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
        "\n",
        "#print(input_sequences[0])\n",
        "\n",
        "# Pad sequences and split inputs/targets\n",
        "# after this X will have inputs and y will have label for those inputs\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# One-hot encode the labels , note- there are other ways for\n",
        "# encoding like pre-trained word2vec encoding and so on\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core of the Transformer model"
      ],
      "metadata": {
        "id": "8cyv455M9V8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads # example - 8\n",
        "\n",
        "        self.embed_dim = embed_dim # example - 512\n",
        "        # embed_dim = dimension of Q, K, and V before splitting into multiple heads\n",
        "        # It is same as total dimension of the input embeddings (word embeddings)\n",
        "\n",
        "        self.projection_dim = embed_dim // num_heads # Size of Each Attention Head's Subspace\n",
        "        # Each head gets a smaller subspace of the embedding dimension\n",
        "        # example - 64\n",
        "\n",
        "        # Fully connected (dense) layers that project the input into Q,K,V\n",
        "        # These layers map the input embeddings to the same embed_dim\n",
        "        # These layers will be reshaped / split later to split across attention heads\n",
        "        # A single large matrix multiplication is more efficient than many small ones\n",
        "        # GPUs love large matrix multiplications because they are optimized for parallel computation\n",
        "        # This allows TF/Keras to efficiently batch the computation, leveraging better GPU memory utilization\n",
        "\n",
        "        self.query_dense = Dense(embed_dim) # Q Determines \"what to focus on\"\n",
        "        self.key_dense = Dense(embed_dim) # K Acts as \"labels\" to be matched with queries\n",
        "        self.value_dense = Dense(embed_dim) # V Holds the actual information\n",
        "\n",
        "        self.combine_heads = Dense(embed_dim)\n",
        "        # After multi-head attention is applied, the outputs from all heads are concatenated back into embed_dim\n",
        "\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        scores = tf.matmul(query, key, transpose_b=True)\n",
        "        scores /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # converting integer to a float32 tensor\n",
        "\n",
        "        attention_probs = tf.nn.softmax(scores, axis=-1) # how much attention each token should give to other tokens\n",
        "        # The higher the score, the more focus that token gets\n",
        "        # Softmax should be applied along the keys (i.e., across the last dimension of the scores matrix)\n",
        "        # Each row corresponds to a query token attending to all key tokens\n",
        "        # This ensures that each query distributes its attention across all keys properly\n",
        "        # Each row sums to 1\n",
        "\n",
        "        return tf.matmul(attention_probs, value), attention_probs\n",
        "\n",
        "    # x - query, key or value with shape - (batch_size, seq_len, embed_dim)\n",
        "    # batch_size - number of sequences being processed in parallel (for batch processing)\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        # before transpose - (batch_size, seq_len, num_heads, projection_dim)\n",
        "        # after transpose - (batch_size, num_heads, seq_len, projection_dim)\n",
        "        # The -1 in tf.reshape is a placeholder that tells TensorFlow to automatically\n",
        "        # infer that dimension's value based on the total number of elements in the tensor\n",
        "        # -1 is replaced by seq_len by tensorflow\n",
        "\n",
        "    # In TF,Keras - call(self, inputs) is a standard method used inside Layer subclasses\n",
        "    # to define the forward pass of a neural network layer\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value = inputs\n",
        "        batch_size = tf.shape(query)[0] # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        query = self.split_heads(self.query_dense(query), batch_size)\n",
        "        key = self.split_heads(self.key_dense(key), batch_size)\n",
        "        value = self.split_heads(self.value_dense(value), batch_size)\n",
        "\n",
        "        attention, _ = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        # before transpose - (batch_size, num_heads, seq_len, projection_dim)\n",
        "        # after transpose -  (batch_size, seq_len, num_heads, projection_dim)\n",
        "\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "        # Merges all heads back into a single vector\n",
        "        # (batch_size, seq_len, num_heads, projection_dim) → (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        return self.combine_heads(concat_attention)\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        # y = (x - mean) / root(variance + epsilon)\n",
        "        # epsilon ensures we never divide by zero\n",
        "        # it is small enough not to affect the result but large enough to prevent instability\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att([inputs, inputs, inputs])\n",
        "\n",
        "        # Dropout randomly deactivates some neurons during training to reduce overfitting\n",
        "        # Ensure dropout is only applied during training, not inference\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output) # Residual Connection\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output) # Residual Connection\n",
        "\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        # The Embedding layer takes an integer tensor and replaces each integer with an embed_dim-sized vector\n",
        "        # example - positions = [0, 1, 2, 3]\n",
        "        # after embedding - positions = [\n",
        "        #   [0.2, 0.1, 0.3, 0.5, 0.6, 0.9, 0.7, 0.8],  # Position 0\n",
        "        #   [0.4, 0.2, 0.1, 0.6, 0.5, 0.7, 0.9, 0.3],  # Position 1\n",
        "        #   [0.5, 0.3, 0.8, 0.2, 0.7, 0.4, 0.6, 0.1],  # Position 2\n",
        "        #   [0.9, 0.6, 0.2, 0.3, 0.1, 0.8, 0.4, 0.7]   # Position 3\n",
        "        # ]\n",
        "\n",
        "        # initial shape of x - (batch_size, seq_len)\n",
        "        # batch_size: Number of sentences in a batch\n",
        "        # seq_len: Number of tokens (words) in each sentence\n",
        "        # Each value in x is an integer index from 0 to vocab_size - 1\n",
        "        # after embedding - (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # example - embed_dim = 8, batch_size = 2\n",
        "        #     x = [\n",
        "        #   [ [0.2, 0.1, 0.4, 0.3, 0.8, 0.7, 0.6, 0.9],  # Token 2\n",
        "        #     [0.5, 0.3, 0.9, 0.1, 0.2, 0.6, 0.8, 0.7],  # Token 5\n",
        "        #     [0.4, 0.9, 0.2, 0.3, 0.1, 0.7, 0.5, 0.6],  # Token 1\n",
        "        #     [0.3, 0.8, 0.6, 0.2, 0.5, 0.9, 0.7, 0.4]   # Token 7\n",
        "        #   ],  # First sentence\n",
        "\n",
        "        #   [ [0.1, 0.6, 0.9, 0.7, 0.3, 0.5, 0.2, 0.8],  # Token 0\n",
        "        #     [0.4, 0.2, 0.3, 0.9, 0.7, 0.5, 0.1, 0.6],  # Token 3\n",
        "        #     [0.8, 0.5, 0.4, 0.1, 0.6, 0.3, 0.2, 0.7],  # Token 8\n",
        "        #     [0.9, 0.3, 0.5, 0.7, 0.8, 0.2, 0.6, 0.1]   # Token 4\n",
        "        #   ]   # Second sentence\n",
        "        # ]\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "        # the maximum sequence length the model can handle\n",
        "        maxlen = tf.shape(x)[-1] # sets maxlen to the length of the input sequence\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1) # Generate [0, 1, 2, ..., maxlen-1]\n",
        "        positions = self.pos_emb(positions) # Each position index is mapped to a trainable embedding of shape (maxlen, embed_dim)\n",
        "        x = self.token_emb(x) # Each token ID in x is mapped to an embedding of shape (batch_size, maxlen, embed_dim)\n",
        "        return x + positions\n",
        "\n",
        "        # x has shape (batch_size, seq_len, embed_dim)\n",
        "        # positions has shape (maxlen, embed_dim)\n",
        "        # But maxlen == seq_len, so positions effectively has shape (seq_len, embed_dim).\n",
        "        # TensorFlow broadcasts positions across batch_size, treating it as if it were (1, seq_len, embed_dim).\n",
        "        # This allows element-wise addition between x and position"
      ],
      "metadata": {
        "id": "LOokkLYy6363"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model the whole architecture , compile and run the training"
      ],
      "metadata": {
        "id": "t7kFCu4f9d58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "embed_dim = 128  # Embedding size\n",
        "num_heads = 4    # Number of attention heads\n",
        "ff_dim = 512     # Feed-forward layer size\n",
        "maxlen = seq_length # here it is 50 defined above\n",
        "\n",
        "# below total words = 6662 (see above - basically all tokens in the text)\n",
        "\n",
        "# Build the model\n",
        "inputs = tf.keras.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, total_words, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "print(x.shape)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x, training=True)\n",
        "print(x.shape)\n",
        "x = x[:, -1, :]\n",
        "print(x.shape)\n",
        "x = Dense(total_words, activation=\"softmax\")(x)\n",
        "print(x.shape)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "gDypn-Vx9SBU",
        "outputId": "5fe355f5-0d4c-430f-9d4a-c67307cd69ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 50, 128)\n",
            "(None, 50, 128)\n",
            "(None, 128)\n",
            "(None, 6663)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m859,264\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m198,272\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item (\u001b[38;5;33mGetItem\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6663\u001b[0m)                │         \u001b[38;5;34m859,527\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">859,264</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6663</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">859,527</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, batch_size=32, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNwZJFVS9Z2B",
        "outputId": "92501e2d-df9d-43c5-d98c-0b5ac86b14b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 110ms/step - accuracy: 0.0820 - loss: 6.5049\n",
            "Epoch 2/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 110ms/step - accuracy: 0.1595 - loss: 5.0777\n",
            "Epoch 3/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 110ms/step - accuracy: 0.2095 - loss: 4.2903\n",
            "Epoch 4/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 110ms/step - accuracy: 0.2578 - loss: 3.6334\n",
            "Epoch 5/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 110ms/step - accuracy: 0.3177 - loss: 3.1068\n",
            "Epoch 6/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 109ms/step - accuracy: 0.3962 - loss: 2.6140\n",
            "Epoch 7/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 109ms/step - accuracy: 0.4798 - loss: 2.1751\n",
            "Epoch 8/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 111ms/step - accuracy: 0.5539 - loss: 1.8186\n",
            "Epoch 9/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 111ms/step - accuracy: 0.6135 - loss: 1.5152\n",
            "Epoch 10/10\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 110ms/step - accuracy: 0.6653 - loss: 1.2756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
        "        seed_text += \" \" + predicted_word\n",
        "    return seed_text\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"harry looked at\"\n",
        "generated_text = generate_text(seed_text, next_words=50, max_sequence_len=seq_length + 1)\n",
        "print(len(generated_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yILfPseH9luo",
        "outputId": "01920780-ba88-4633-ade7-583b26fd7c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFYBNNIWbA_T",
        "outputId": "c0d3c830-1be2-4f1e-8308-6b2748653112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harry looked at the first time he got some wizarding families what’s the matter with him without it in the air ” said harry ron and hermione were in danger he himself darted around the board taking almost as many white pieces as they walked off the board leaving three sparkling with a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Is Missing Compared to ChatGPT?\n",
        "\n",
        "- Masked Attention:\n",
        "\n",
        "ChatGPT uses causal masking so that a word cannot see future words during training. Our model uses regular attention, which allows it to see the entire sequence.\n",
        "\n",
        "- Multiple Stacked Transformer Blocks:\n",
        "\n",
        "ChatGPT has many layers (e.g., 12, 24, 96 layers). Our model has only one Transformer block.\n",
        "\n",
        "- Tokenization & Byte-Pair Encoding (BPE):\n",
        "\n",
        "ChatGPT does not use simple tokenization; it uses Byte-Pair Encoding (BPE) or WordPiece for better vocabulary handling. Our model uses basic word tokenization.\n",
        "\n",
        "- Training on Large Datasets:\n",
        "\n",
        "ChatGPT is trained on hundreds of GBs of text. Our model is trained on a single Harry Potter book (very limited).\n",
        "\n",
        "- Decoding Strategies for Text Generation:\n",
        "\n",
        "ChatGPT uses sampling (top-k, nucleus sampling) or beam search to generate text. Our model does not have a decoding strategy."
      ],
      "metadata": {
        "id": "HCHYzhnSn0J6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cchO78wTbKIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}